{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing used libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from ipywidgets import interact, fixed\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import category_encoders as ce\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import plot_importance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from pdpbox.pdp import pdp_interact, pdp_interact_plot\n",
    "import shap\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "files = [f for f in glob.glob(path + \"*/**/*.json\", recursive=True)]\n",
    "\n",
    "def json_to_df(json_path):\n",
    "    with open(json_path, 'r') as json_data:\n",
    "        data = json.load(json_data)\n",
    "    data = pd.DataFrame.from_dict(data, orient='index')\n",
    "    data = data[0]\n",
    "    mapid = data['id']\n",
    "    match_name = data['NameWithoutExtension']\n",
    "    map_name = data['map_name']\n",
    "    team1_series_score = data['score_team1']\n",
    "    team2_series_score = data['score_team2']\n",
    "    team1_half1_score = data['score_half1_team1']\n",
    "    team2_half1_score = data['score_half1_team2']\n",
    "    team2_half2_score = data['score_half2_team2']\n",
    "    team2_half1_score = data['score_half2_team1']\n",
    "    team1_name = pd.DataFrame.from_dict(data['team_ct'], orient='index')[0][0]\n",
    "    team2_name = pd.DataFrame.from_dict(data['team_t'], orient='index')[0][0]\n",
    "\n",
    "\n",
    "    team_ct_df = pd.DataFrame.from_dict(data['team_ct'], orient='index')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ct_start_money_df = pd.DataFrame()\n",
    "    ct_equipment_value_df = pd.DataFrame()\n",
    "    ct_money_earned_df = pd.DataFrame()\n",
    "    ct_time_death_rounds = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,5):\n",
    "        ct_start_money_df[i] = pd.DataFrame.from_dict(pd.DataFrame(team_ct_df[0][4])['start_money_rounds'][i], orient='index')[0]\n",
    "        ct_equipment_value_df[i] = pd.DataFrame.from_dict(pd.DataFrame(team_ct_df[0][4])['equipement_value_rounds'][i], orient='index')[0]\n",
    "        ct_money_earned_df[i] = pd.DataFrame.from_dict(pd.DataFrame(team_ct_df[0][4])['rounds_money_earned'][i], orient='index')[0]\n",
    "        ct_time_death_rounds[i] = pd.DataFrame.from_dict(pd.DataFrame(team_ct_df[0][4])['time_death_rounds'][i], orient='index')[0]\n",
    "\n",
    "\n",
    "    ct_start_money_df['Total'] = ct_start_money_df.sum(axis=1)\n",
    "    ct_equipment_value_df['Total'] = ct_equipment_value_df.sum(axis=1)\n",
    "    ct_money_earned_df['ct_earnedmoney_total'] = ct_money_earned_df.sum(axis=1)\n",
    "\n",
    "    ct_start_money_df.index = ct_start_money_df.index.rename('round')\n",
    "    ct_start_money_df = ct_start_money_df.reset_index()\n",
    "\n",
    "    ct_equipment_value_df.index = ct_equipment_value_df.index.rename('round')\n",
    "    ct_equipment_value_df = ct_equipment_value_df.reset_index()\n",
    "\n",
    "    ct_money_earned_df.index = ct_money_earned_df.index.rename('round')\n",
    "    ct_money_earned_df = ct_money_earned_df.reset_index()\n",
    "\n",
    "    ct_time_death_rounds['ct_survived'] = ct_time_death_rounds.isin(['0.0']).sum(axis=1)\n",
    "    ct_time_death_rounds.index = ct_time_death_rounds.index.rename('round')\n",
    "    ct_time_death_rounds = ct_time_death_rounds.reset_index()\n",
    "    ct_time_death_rounds['round'] = ct_time_death_rounds['round'].astype(int) \n",
    "    \n",
    "\n",
    "\n",
    "    team_t_df = pd.DataFrame.from_dict(data['team_t'], orient='index')\n",
    "    t_start_money_df = pd.DataFrame()\n",
    "    t_equipment_value_df = pd.DataFrame()\n",
    "    t_money_earned_df = pd.DataFrame()\n",
    "    t_time_death_rounds = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,5):\n",
    "        t_start_money_df[i] = pd.DataFrame.from_dict(pd.DataFrame(team_t_df[0][4])['start_money_rounds'][i], orient='index')[0]\n",
    "        t_equipment_value_df[i] = pd.DataFrame.from_dict(pd.DataFrame(team_t_df[0][4])['equipement_value_rounds'][i], orient='index')[0]\n",
    "        t_money_earned_df[i] = pd.DataFrame.from_dict(pd.DataFrame(team_t_df[0][4])['rounds_money_earned'][i], orient='index')[0]\n",
    "        t_time_death_rounds[i] = pd.DataFrame.from_dict(pd.DataFrame(team_t_df[0][4])['time_death_rounds'][i], orient='index')[0]\n",
    "\n",
    "\n",
    "    t_start_money_df['Total'] = t_start_money_df.sum(axis=1)\n",
    "    t_equipment_value_df['Total'] = t_equipment_value_df.sum(axis=1)\n",
    "    t_money_earned_df['t_earnedmoney_total'] = t_money_earned_df.sum(axis=1)\n",
    "\n",
    "    t_start_money_df.index = t_start_money_df.index.rename('round')\n",
    "    t_start_money_df = t_start_money_df.reset_index()\n",
    "\n",
    "    t_equipment_value_df.index = t_equipment_value_df.index.rename('round')\n",
    "    t_equipment_value_df = t_equipment_value_df.reset_index()\n",
    "\n",
    "    t_money_earned_df.index = t_money_earned_df.index.rename('round')\n",
    "    t_money_earned_df = t_money_earned_df.reset_index()\n",
    "\n",
    "    t_time_death_rounds['t_survived'] = t_time_death_rounds.isin(['0.0']).sum(axis=1)\n",
    "    t_time_death_rounds.index = t_time_death_rounds.index.rename('round')\n",
    "    t_time_death_rounds = t_time_death_rounds.reset_index()\n",
    "    t_time_death_rounds['round'] = t_time_death_rounds['round'].astype(int) \n",
    "    \n",
    "\n",
    "    \n",
    "    allrounds = data.loc['rounds']\n",
    "    allrounds = pd.DataFrame(allrounds)\n",
    "    \n",
    "    roundsdf = pd.DataFrame()\n",
    "    roundsdf['round'] = allrounds['number']\n",
    "    roundsdf['winner_side'] = allrounds['winner_side']\n",
    "    roundsdf['winner_name'] = allrounds['winner_name']\n",
    "    roundsdf['team_t_name'] = allrounds['team_t_name']\n",
    "    roundsdf['team_ct_name'] = allrounds['team_ct_name']\n",
    "    roundsdf['equipment_value_team_t'] = allrounds['equipement_value_team_t']\n",
    "    roundsdf['equipment_value_team_ct'] = allrounds['equipement_value_team_ct']\n",
    "    roundsdf['start_money_team_t'] = allrounds['start_money_team_t']\n",
    "    roundsdf['start_money_team_ct'] = allrounds['start_money_team_ct']\n",
    "    roundsdf['bomb_defused_count'] = allrounds ['bomb_defused_count']\n",
    "    roundsdf['bomb_exploded_count'] = allrounds['bomb_exploded_count'] \n",
    "    roundsdf['bomb_planted_count'] = allrounds['bomb_planted_count'] \n",
    "    roundsdf['duration'] = allrounds['duration']\n",
    "    roundsdf['type'] = allrounds['type']\n",
    "    \n",
    "    roundsdf = pd.merge(roundsdf,ct_time_death_rounds[['round','ct_survived']],on='round',how='left')\n",
    "    roundsdf = pd.merge(roundsdf,t_time_death_rounds[['round','t_survived']],on='round',how='left')\n",
    "    \n",
    "    ct_money_earned_df['round'] = ct_money_earned_df['round'].astype(int)\n",
    "    ct_money_earned_df[['round','ct_earnedmoney_total']]\n",
    "    t_money_earned_df['round'] = t_money_earned_df['round'].astype(int)\n",
    "    t_money_earned_df[['round','t_earnedmoney_total']]\n",
    "    \n",
    "        \n",
    "    roundsdf['winner_side'] = roundsdf['winner_side'].replace('CT',0)\n",
    "    roundsdf['winner_side'] = roundsdf['winner_side'].replace('T',1)\n",
    "    \n",
    "    roundsdf['mapid'] = mapid\n",
    "    roundsdf['match_name'] = match_name\n",
    "    roundsdf['map_name'] = map_name\n",
    "    \n",
    "    roundsdf['team1_series_score'] = team1_series_score\n",
    "    roundsdf['team2_series_score']= team2_series_score\n",
    "       \n",
    "    return roundsdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "train_path = str(path)+\"/train_majors/\"\n",
    "all_files = [f for f in glob.glob(train_path + \"**/**.csv\", recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    filepath = f.split('.dem.')[0]\n",
    "    df_json = json_to_df(f)\n",
    "    df_json.to_csv(str(filepath)+\".csv\")\n",
    "    print(len(df_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_files = [f for f in glob.glob(path + \"*/**/*.csv\", recursive=True)]\n",
    "# for f in all_files:\n",
    "#     os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-538315e1d00c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtraindf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtraindf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraindf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtraindf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraindf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraindf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'winner_side'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;34m'SPEC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No objects to concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "train_path = str(path)+\"/train_majors/\"\n",
    "train_files = [f for f in glob.glob(train_path + \"**/**.csv\", recursive=True)]\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in train_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "traindf = pd.concat(li, axis=0, ignore_index=True)\n",
    "traindf = traindf.drop('Unnamed: 0',axis=1)\n",
    "traindf = traindf[traindf['winner_side'] !='SPEC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "train_path = str(path)+\"/test_major/\"\n",
    "train_files = [f for f in glob.glob(train_path + \"**/**.csv\", recursive=True)]\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in train_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "testdf = pd.concat(li, axis=0, ignore_index=True)\n",
    "testdf = testdf.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_csvs(df):\n",
    "    #rearrange the columns\n",
    "    df_cols = ['round', 'equipment_value_team_t', 'equipment_value_team_ct','start_money_team_t',\n",
    "        'start_money_team_ct','bomb_defused_count','bomb_exploded_count','bomb_planted_count','duration',\n",
    "               'type', 'ct_survived', 't_survived', 'mapid', 'match_name', 'map_name',\n",
    "            'team1_series_score', 'team2_series_score', 'winner_name', 'team_t_name', 'team_ct_name',\n",
    "            'winner_side']\n",
    "    df = df[df_cols]\n",
    "    #copying the dataframe to avoid warning messages\n",
    "    df = df.copy()\n",
    "    \n",
    "    \n",
    "    #initializing the series columns\n",
    "    df['team1_current_score'] = 0 * len(df)\n",
    "    df['team2_current_score'] = 0 * len(df)\n",
    "    #grabbing the amount of rows in the dataframe\n",
    "    rows, _ = df.shape\n",
    "    #dummyvalue for first entry in the dataframe\n",
    "    lastmatch = 'newdf'\n",
    "    \n",
    "    for arow in range(0,rows):\n",
    "        #assign the winner of the round and matchid\n",
    "        roundwinner = df.iloc[arow]['winner_name']\n",
    "        roundmapid = df.iloc[arow]['mapid']\n",
    "        if roundmapid != lastmatch:\n",
    "            #if the matchid does not match the previous row's matchid\n",
    "            #this is a new series\n",
    "            \n",
    "            #assigning the team names\n",
    "            team2_name = df.iloc[arow]['team_t_name']\n",
    "            team1_name = df.iloc[arow]['team_ct_name']          \n",
    "            if roundwinner == team1_name:\n",
    "                df.at[arow,'team1_current_score'] = 1\n",
    "                lastmatch = df.iloc[arow]['mapid']\n",
    "            else:\n",
    "                df.at[arow,'team2_current_score']= 1\n",
    "                lastmatch = df.iloc[arow]['mapid']\n",
    "        else:\n",
    "            #if it is the same match, pulling the last row allows us to\n",
    "            #append to the previous series score\n",
    "            lastrow = df.iloc[(arow-1)]\n",
    "            old_t1_score = lastrow['team1_current_score'].astype(int)\n",
    "            old_t2_score = lastrow['team2_current_score'].astype(int)\n",
    "            \n",
    "            if df.iloc[arow]['winner_name'] == team1_name:\n",
    "                lastmatch = df.iloc[arow]['mapid']\n",
    "                df.at[arow,'team1_current_score']=(old_t1_score+1)\n",
    "                df.at[arow,'team2_current_score']=(old_t2_score)\n",
    "            else:\n",
    "                lastmatch = df.iloc[arow]['mapid']\n",
    "                df.at[arow,'team1_current_score']=(old_t1_score)\n",
    "                df.at[arow,'team2_current_score']=(old_t2_score+1)\n",
    "                \n",
    "    #measuring the net surplus each team has in equipment value at the start of the round\n",
    "    \n",
    "    \n",
    "    df['equipment_value_team_t_surplus'] = df['equipment_value_team_t']-df['equipment_value_team_ct']\n",
    "    df['equipment_value_team_ct_surplus'] = df['equipment_value_team_ct']-df['equipment_value_team_t']\n",
    "    df['start_money_team_t_surplus'] = df['start_money_team_t'] - df['start_money_team_ct']\n",
    "    df['start_money_team_ct_surplus'] = df['start_money_team_ct'] - df['start_money_team_t']\n",
    " \n",
    "    ##this creates the value of a team's equipment last round\n",
    "    df['equipment_value_team_t_surplus_LR'] = df['equipment_value_team_t_surplus'].shift(1)\n",
    "    df['equipment_value_team_ct_surplus_LR'] = df['equipment_value_team_ct_surplus'].shift(1)\n",
    "    df['equipment_value_team_ct_surplus_LR'][df['round']==1] = np.NaN\n",
    "    df['equipment_value_team_t_surplus_LR'][df['round']==1] = np.NaN\n",
    "    df['equipment_value_team_ct_surplus_LR'][df['round']==16] = np.NaN\n",
    "    df['equipment_value_team_t_surplus_LR'][df['round']==16] = np.NaN\n",
    "    df['equipment_value_team_ct_surplus_LR'][df['round']==31] = np.NaN\n",
    "    df['equipment_value_team_t_surplus_LR'][df['round']==31] = np.NaN\n",
    "    df['equipment_value_team_ct_surplus_LR'][df['round']==46] = np.NaN\n",
    "    df['equipment_value_team_t_surplus_LR'][df['round']==46] = np.NaN\n",
    "    \n",
    "    \n",
    "    ##this creates the value of economy last round\n",
    "    df['start_money_team_t_surplus_LR'] = df['start_money_team_t_surplus'].shift(1)\n",
    "    df['start_money_team_ct_surplus_LR'] = df['start_money_team_ct_surplus'].shift(1)\n",
    "    df['start_money_team_ct_surplus_LR'][df['round']==1] = np.NaN\n",
    "    df['start_money_team_t_surplus_LR'][df['round']==1] = np.NaN\n",
    "    df['start_money_team_ct_surplus_LR'][df['round']==16] = np.NaN\n",
    "    df['start_money_team_t_surplus_LR'][df['round']==16] = np.NaN\n",
    "    df['start_money_team_ct_surplus_LR'][df['round']==31] = np.NaN\n",
    "    df['start_money_team_t_surplus_LR'][df['round']==31] = np.NaN\n",
    "    df['start_money_team_ct_surplus_LR'][df['round']==46] = np.NaN\n",
    "    df['start_money_team_t_surplus_LR'][df['round']==46] = np.NaN    \n",
    "        \n",
    "    ##this creates the length of last round as a feature\n",
    "    df['lastround_duration'] = df['duration'].shift(1)\n",
    "    df['lastround_duration'][df['round']==1] = np.NaN\n",
    "    \n",
    "    \n",
    "    \n",
    "    #this creates number survived last round\n",
    "    df['ct_survived_LR'] = df['ct_survived'].shift(1)\n",
    "    df['t_survived_LR'] = df['t_survived'].shift(1)\n",
    "    df['ct_survived_LR'][df['round']==1] = np.NaN\n",
    "    df['t_survived_LR'][df['round']==16] = np.NaN\n",
    "    df['ct_survived_LR'][df['round']==1] = np.NaN\n",
    "    df['t_survived_LR'][df['round']==16] = np.NaN\n",
    "    df['ct_survived_LR'][df['round']==30] = np.NaN\n",
    "    df['t_survived_LR'][df['round']==30] = np.NaN\n",
    "    df['ct_survived_LR'][df['round']==45] = np.NaN\n",
    "    df['t_survived_LR'][df['round']==45] = np.NaN\n",
    "    \n",
    "    df['team1_current_score_LR'] = df['team1_current_score'].shift(1)\n",
    "    df['team2_current_score_LR'] = df['team2_current_score'].shift(1)\n",
    "    df['team1_current_score_LR'][df['round']==1] = np.NaN\n",
    "    df['team2_current_score_LR'][df['round']==1] = np.NaN\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##this creates the value of economy last round\n",
    "    df['t_win_LR'] = df['winner_side'].shift(1)\n",
    "    df['t_win_LR'][df['round']==1] = np.NaN\n",
    "    df['t_win_LR']=df['t_win_LR'].astype(float)\n",
    "    \n",
    "     \n",
    "    map_name = df['map_name']\n",
    "    map_name = map_name.replace('workshop/125438669/de_inferno','de_inferno')\n",
    "    map_name = map_name.replace('workshop/125438255/de_dust2','de_dust2')\n",
    "    map_name = map_name.replace('workshop/152508932/de_mirage ','de_mirage')\n",
    "    map_name = map_name.replace('workshop/125439125/de_nuke','de_nuke')\n",
    "    map_name = map_name.replace('workshop/125438372/de_train','de_train')\n",
    "    map_name = map_name.replace('workshop/152508932/de_mirage','de_mirage')\n",
    "    df['map_name'] = map_name\n",
    "    df = df[df['map_name']!='de_vertigo']\n",
    "    df['eqipment_value_ct_swing'] = df['equipment_value_team_ct_surplus'] - df['start_money_team_ct_surplus_LR']\n",
    "    df['eqipment_value_t_swing'] = df['equipment_value_team_t_surplus'] - df['start_money_team_t_surplus_LR']\n",
    "    df = df[~df['winner_side'].isna()]\n",
    "    df['t_win_LR'] = df['t_win_LR'].astype(float)\n",
    "    df['winner_side'] = df['winner_side'].astype(float)\n",
    "    \n",
    "    rows, _ = df.shape\n",
    "    for arow in range(0,rows):\n",
    "        #assign the winner of the round and matchid\n",
    "        roundnumber = df.iloc[arow]['round']\n",
    "        side = roundnumber//16\n",
    "        team1score = df.iloc[arow]['team1_current_score_LR']\n",
    "        team2score = df.iloc[arow]['team2_current_score_LR']\n",
    "        if side == 0:\n",
    "            df.at[arow,'ct_score_LR'] = team1score\n",
    "            df.at[arow,'t_score_LR'] = team2score\n",
    "        elif side == 1:\n",
    "            df.at[arow,'ct_score_LR'] = team2score\n",
    "            df.at[arow,'t_score_LR'] = team1score\n",
    "        elif side == 2:\n",
    "            df.at[arow,'ct_score_LR'] = team1score\n",
    "            df.at[arow,'t_score_LR'] = team2score\n",
    "        elif side == 3:\n",
    "            df.at[arow,'ct_score_LR'] = team2score\n",
    "            df.at[arow,'t_score_LR'] = team1score\n",
    "        else:\n",
    "            df.at[arow,'ct_score_LR'] = team1score\n",
    "            df.at[arow,'t_score_LR'] = team2score        \n",
    "    df['ct_score_LR'][df['round']==1] = np.NaN\n",
    "    df['ct_score_LR'][df['round']==1] = np.NaN\n",
    "    df = df[~df['winner_side'].isna()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows, _ = df.shape\n",
    "# for arow in range(0,rows):\n",
    "#     #assign the winner of the round and matchid\n",
    "#     roundnumber = df.iloc[arow]['round']\n",
    "#     side = roundnumber//16\n",
    "#     team1score = df.iloc[arow]['team1_current_score_LR']\n",
    "#     team2score = df.iloc[arow]['team2_current_score_LR']\n",
    "#     if side == 0:\n",
    "#         df['ct_score_LR'] = team1score\n",
    "#         df['t_score_LR'] = team2score\n",
    "#     elif side == 1:\n",
    "#         df['ct_score_LR'] = team2score\n",
    "#         df['t_score_LR'] = team1score\n",
    "#     elif side == 2:\n",
    "#         df['ct_score_LR'] = team1score\n",
    "#         df['t_score_LR'] = team2score\n",
    "#     elif side == 3:\n",
    "#         df['ct_score_LR'] = team2score\n",
    "#         df['t_score_LR'] = team1score\n",
    "#     elif side == 4:\n",
    "#         df['ct_score_LR'] = team1score\n",
    "#         df['t_score_LR'] = team2score        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = format_csvs(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = format_csvs(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['round','equipment_value_team_t','equipment_value_team_ct','ct_score_LR','t_score_LR','map_name']\n",
    "target = 'winner_side'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.fillna(-99)\n",
    "# test_df = test_df.fillna(-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target]\n",
    "\n",
    "#encoding the one None as CT Win\n",
    "#drop match or fix later\n",
    "#y_all = y_all.replace('None',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the train set into an 80/20 train/validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25,random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train, y_train, test_size=0.375,random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodin = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True), \n",
    "    XGBClassifier(max_depth=3, \n",
    "                  n_estimators=200,\n",
    "                  n_jobs=-1,\n",
    "                  learning_rate=0.02,\n",
    "                  random_state=1337,\n",
    "                  eval_set=eval_set,\n",
    "                  eval_metric='auc',\n",
    "                  early_stopping_rounds=25)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_set = [(X_train_transformed,y_train),(X_val_transformed,y_val)]\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1,2,3,5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5,6],\n",
    "        'n_estimators': [100,150,200,300,500],\n",
    "        #'learning_rate': np.random.uniform(0.01,0.2)\n",
    "        }\n",
    "\n",
    "k = 3\n",
    "param_comb = 500\n",
    "\n",
    "transformers = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True)\n",
    ")\n",
    "\n",
    "X_train_transformed = transformers.fit_transform(X_train)\n",
    "X_val_transformed = transformers.transform(X_val)\n",
    "X_test_transformed = transformers.transform(X_test)\n",
    "\n",
    "\n",
    "model = XGBClassifier(eval_set=eval_set,eval_metric='auc',early_stopping_rounds=50)\n",
    "\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(model, \n",
    "                                   param_distributions=params, \n",
    "                                   n_iter=param_comb, \n",
    "                                   scoring='roc_auc', \n",
    "                                   n_jobs=-1, \n",
    "                                   cv=k, \n",
    "                                   verbose=3,\n",
    "                                   random_state=1337 )\n",
    "random_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters', random_search.best_params_)\n",
    "print('Cross-validation ROC AUC', random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = transformers.fit_transform(X_train)\n",
    "X_val_transformed = transformers.transform(X_val)\n",
    "X_test_transformed = transformers.transform(X_test)\n",
    "\n",
    "pipelinebest = random_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need the proba function (probability of being classified as a 1 to do aucroc analysis)\n",
    "#this is why we use the .predict_proba and not just the class outcome in .predict\n",
    "#we select all the values of the second column (positive likelyhook) for the auc curve\n",
    "\n",
    "y_pred_proba = pipelinebest.predict_proba(X_test_transformed)[:, 1]\n",
    "y_val_pred_proba = pipelinebest.predict(X_val_transformed)\n",
    "\n",
    "\n",
    "print('Validation Accuracy', accuracy_score(y_val_pred_proba, y_val))\n",
    "print('\\nTest ROC AUC:', roc_auc_score(y_test, y_pred_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "xgb = pipeline.named_steps['xgbclassifier']\n",
    "ohe = pipeline.named_steps['onehotencoder']\n",
    "\n",
    "\n",
    "importances = pd.Series(xgb.feature_importances_, ohe.feature_names)\n",
    "\n",
    "# Plot feature importances\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 20\n",
    "plt.figure(figsize=(10,n/2))\n",
    "plt.title(f'Top {n} features')\n",
    "importances.sort_values()[-n:].plot.barh(color='grey');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in X_test.columns:\n",
    "#     # Fit without column\n",
    "#     pipeline = make_pipeline(\n",
    "#         ce.OneHotEncoder(use_cat_names=True), \n",
    "#         XGBClassifier(max_depth=4, n_estimators=200,n_jobs=-1, random_state=1337,eval_set=eval_set,eval_metric='auc',early_stopping_rounds=100)\n",
    "#     )\n",
    "#     pipeline.fit(X_train.drop(columns=column), y_train)\n",
    "#     score_without = pipeline.score(X_val.drop(columns=column), y_val)\n",
    "#     print(f'Validation Accuracy without {column}: {score_without}')\n",
    "\n",
    "#     # Fit with column\n",
    "#     pipeline = make_pipeline(\n",
    "#         ce.OneHotEncoder(use_cat_names=True), \n",
    "#         XGBClassifier(max_depth=4, n_estimators=200,n_jobs=-1, random_state=1337,eval_set=eval_set,eval_metric='auc',early_stopping_rounds=100)\n",
    "#     )\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "#     score_with = pipeline.score(X_val, y_val)\n",
    "#     print(f'Validation Accuracy with {column}: {score_with}')\n",
    "\n",
    "#     # Compare the error with & without column\n",
    "#     print(f'Drop-Column Importance for {column}: {score_with - score_without}')\n",
    "#     print('\\n')\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(max_depth=3, \n",
    "                  n_estimators=200,\n",
    "                  n_jobs=-1,\n",
    "                  learning_rate=0.02,\n",
    "                  random_state=1337,\n",
    "                  eval_set=eval_set,\n",
    "                  eval_metric='auc',\n",
    "                  early_stopping_rounds=25)\n",
    "\n",
    "transformers = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True)\n",
    ")\n",
    "\n",
    "X_train_transformed = transformers.fit_transform(X_train)\n",
    "X_val_transformed = transformers.transform(X_val)\n",
    "X_test_transformed = transformers.transform(X_test)\n",
    "\n",
    "\n",
    "model.fit(X_train_transformed,y_train)\n",
    "\n",
    "permuter = PermutationImportance(\n",
    "    model,\n",
    "    scoring='accuracy', \n",
    "    n_iter=10, \n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#oldmodel\n",
    "#model = XGBClassifier(max_depth=4, n_estimators=100,n_jobs=-1, random_state=1337,eval_set=eval_set,eval_metric='auc',early_stopping_rounds=50)\n",
    "\n",
    "\n",
    "\n",
    "#dropping pistol rounds for PDP/PI analysis\n",
    "PI_X_train_transformed = X_train_transformed.fillna(0)\n",
    "PI_X_val_transformed = X_val_transformed.fillna(0)\n",
    "PI_X_test_transformed = X_test_transformed.fillna(0)\n",
    "\n",
    "\n",
    "permuter.fit(PI_X_val_transformed, y_val)\n",
    "feature_names = PI_X_val_transformed.columns.tolist()\n",
    "\n",
    "eli5.show_weights(\n",
    "    permuter, \n",
    "    top=None, # show permutation importances for all features\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = ['equipment_value_team_t','equipment_value_team_ct']\n",
    "\n",
    "interaction = pdp_interact(\n",
    "    model=model, \n",
    "    dataset=PI_X_val_transformed, \n",
    "    model_features=PI_X_val_transformed.columns, \n",
    "    features=features\n",
    ")\n",
    "\n",
    "pdp_interact_plot(interaction, plot_type='grid', feature_names=features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdpbox.pdp import pdp_interact, pdp_interact_plot\n",
    "\n",
    "features = ['t_score_LR','ct_score_LR']\n",
    "\n",
    "interaction = pdp_interact(\n",
    "    model=model, \n",
    "    dataset=PI_X_val_transformed, \n",
    "    model_features=PI_X_val_transformed.columns, \n",
    "    features=features\n",
    ")\n",
    "\n",
    "pdp_interact_plot(interaction, plot_type='grid', feature_names=features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = ['map_name_de_nuke', 'map_name_de_dust2','map_name_de_mirage','map_name_de_overpass','map_name_de_inferno','map_name_de_cbble','map_name_de_cache','map_name_de_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for amap in maps:\n",
    "\n",
    "    features = ['equipment_value_team_t',amap]\n",
    "\n",
    "    interaction = pdp_interact(\n",
    "        model=model, \n",
    "        dataset=PI_X_test_transformed, \n",
    "        model_features=PI_X_test_transformed.columns, \n",
    "        features=features\n",
    "    )\n",
    "\n",
    "    pdp_interact_plot(interaction, plot_type='grid', feature_names=features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(X_train,y_train),(X_val,y_val)]\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True), \n",
    "    XGBClassifier(max_depth=4, n_estimators=100,n_jobs=-1, random_state=1337,eval_set=eval_set,eval_metric='auc',early_stopping_rounds=50)\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline[-1]       # get the last step in the pipeline\n",
    "processor = pipeline[:-1]  # get all steps except for the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = X_test.iloc[[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['winner_side'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "row_processed = processor.transform(row)\n",
    "shap_values = explainer.shap_values(row_processed)\n",
    "\n",
    "shap.initjs()\n",
    "print(\"Expected value is:\"+str(explainer.expected_value)),\n",
    "shap.force_plot(\n",
    "    base_value=explainer.expected_value, \n",
    "    shap_values=shap_values, \n",
    "    features=row_processed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['y_pred_proba'] = model.predict_proba(X_test_transformed)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate = pd.DataFrame({\n",
    "    'index': test_df.index, \n",
    "    'pred_proba': test_df['y_pred_proba'], \n",
    "    't_win': test_df['winner_side']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_index = X_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate = df_evaluate.merge(\n",
    "    X_test_index,\n",
    "    how='left',\n",
    "    on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test['winner_side']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_win = y_test == 1\n",
    "ct_win = ~t_win\n",
    "right = (t_win) == (df_evaluate['pred_proba'] > 0.50)\n",
    "wrong = ~right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate[t_win & right].sort_values(by='pred_proba',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate[t_win & wrong].sort_values(by='pred_proba',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate[ct_win & right].sort_values(by='pred_proba',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate[ct_win & wrong].sort_values(by='pred_proba',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(pipeline, 'pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to had\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
